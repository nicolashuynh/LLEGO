{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaloBoost Experiments (arxiv)\n",
    "\n",
    "This script analyzes the performance of PaloBoost (Gradient Boosting with Pruning and Adaptive Learning Rate using Out-of-Bag Samples) against to Scikit-learn GBM and XGBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "from bonsai.ensemble.paloboost import PaloBoost\n",
    "from bonsai.ensemble.gbm import GBM\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import preprocessing as pp\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reg_perf(models, X_train ,y_train, X_test, y_test):\n",
    "    performance = {}\n",
    "    for name, model in models.items():\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        time_fit = time.time() - start\n",
    "        print(\"  {0}: {1:.5f} sec...\".format(name, time_fit))\n",
    "        performance[name] = []\n",
    "        if \"XGBoost\" not in name:\n",
    "            for i, y_hat_i in enumerate(model.staged_predict(X_test)):\n",
    "                performance[name].append(np.clip(r2_score(y_test, y_hat_i), 0, 1))\n",
    "        else:\n",
    "            for i in range(n_estimators):\n",
    "                y_hat_i = model.predict(X_test, ntree_limit=i+1)\n",
    "                performance[name].append(np.clip(r2_score(y_test, y_hat_i), 0, 1))\n",
    "    perf_df = pd.DataFrame(performance)\n",
    "    perf_df.columns = [\"{0} ({1:.5f}*)\".format(c, v) \n",
    "                       for c, v in zip(perf_df.columns, perf_df.max())]\n",
    "    return perf_df\n",
    "    \n",
    "def get_cls_perf(models, X_train, y_train, X_test, y_test):\n",
    "    performance = {}\n",
    "    for name, model in models.items():\n",
    "        start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        time_fit = time.time() - start\n",
    "        print(\"  {0}: {1:.5f} sec...\".format(name, time_fit))\n",
    "        performance[name] = []        \n",
    "        if \"XGBoost\" not in name:\n",
    "            for i, y_hat_i in enumerate(model.staged_predict_proba(X_test)):\n",
    "                performance[name].append(roc_auc_score(y_test, y_hat_i[:,1]))\n",
    "        else:\n",
    "            for i in range(n_estimators):\n",
    "                y_hat_i = model.predict_proba(X_test, ntree_limit=i+1)\n",
    "                performance[name].append(roc_auc_score(y_test, y_hat_i[:,1]))\n",
    "    perf_df = pd.DataFrame(performance)\n",
    "    perf_df.columns = [\"{0} ({1:.5f}*)\".format(c, v) \n",
    "                       for c, v in zip(perf_df.columns, perf_df.max())]\n",
    "    return perf_df\n",
    "\n",
    "def plot_perf(perf_df, task, proj, ylim=None):\n",
    "    ax = perf_df.plot(figsize=(6, 4), style=[\"-\",\"--\",\":\",\"-.\"], lw=2.5, ylim=ylim)\n",
    "    if task == \"reg\":\n",
    "        ax.set(xlabel=\"Prediction stage\", ylabel=\"R-squared\")\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(\"figures/{}_{}_ss{}_lr{}.pdf\".format(task, proj, subsample, learning_rate))\n",
    "    else:\n",
    "        ax.set(xlabel=\"Prediction stage\", ylabel=\"AUROC\")\n",
    "        fig = ax.get_figure()\n",
    "        fig.savefig(\"figures/{}_{}_ss{}_lr{}.pdf\".format(task, proj, subsample, learning_rate))\n",
    "        \n",
    "def plot_prune_ratio(models, task, proj):\n",
    "    paloboost = models[\"0. PaloBoost    \"]\n",
    "    prune_df = pd.DataFrame(paloboost.get_prune_stats())\n",
    "    prune_ratio = (prune_df[1] - prune_df[2])/prune_df[1]\n",
    "    ax = prune_ratio.plot(figsize=(6, 4), alpha=0.5)\n",
    "    rolling = prune_ratio.rolling(window=20).mean().plot(ax=ax, lw=2.5, alpha=1)\n",
    "    ax.set(xlabel=\"Prediction stage\", ylabel=\"Prune Ratio\")\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"figures/{}_{}_ss{}_lr{}_prrt.pdf\".format(task, proj, subsample, learning_rate))\n",
    "\n",
    "def plot_avg_lr(models, task, proj):\n",
    "    paloboost = models[\"0. PaloBoost    \"]\n",
    "    lr_df = pd.DataFrame(paloboost.get_lr_stats())\n",
    "    ax = lr_df[1].plot(figsize=(6, 4), alpha=0.5)\n",
    "    rolling = lr_df[1].rolling(window=20).mean().plot(ax=ax, lw=2.5, alpha=1)\n",
    "    ax.set(xlabel=\"Prediction stage\", ylabel=\"Average Learning Rate\")\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"figures/{}_{}_ss{}_lr{}_avglr.pdf\".format(task, proj, subsample, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_estimators = 200\n",
    "learning_rate = 1.0 # 1.0, 0.5, 0.1\n",
    "test_size = 0.7  # 30% training, 70% test - to highlight the overfitting aspect of the models\n",
    "subsample = 0.7\n",
    "max_depth = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Task - Mercedes Challenge\n",
    "\n",
    "We use the dataset from [one of the Kaggle competitions - Mercedes-Benz Greener Manufacturing](https://www.kaggle.com/c/mercedes-benz-greener-manufacturing). We minimally preprocess the data - for more information about how we preprocess the data, please read `preprocess.py` in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/kaggle-mercedes/train.csv\")\n",
    "col_names = data.columns\n",
    "col_names_x = [cname for cname in col_names if cname not in [\"ID\", \"y\"]]\n",
    "X = pp.simple_pp(data[col_names_x]).values\n",
    "y = data[\"y\"].values\n",
    "print(\"- Avg(y): {}, Std(y): {}\".format(np.mean(y), np.std(y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size = test_size,\n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of our benchmark models is as follows:\n",
    "- `paloboost`: PaloBoost (GBM with Pruning and Adaptive Learning Rate)\n",
    "- `gbm`: GBM implemented with Bonsai decision trees\n",
    "- `xgboost`: XGBoost with Scikit-earn interface\n",
    "- `sklearn`: Scikit Learn GBM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"0. PaloBoost    \": PaloBoost(distribution=\"gaussian\",\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "        \"1. SGTB-Bonsai\": GBM(distribution=\"gaussian\",\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "        \"2. XGBoost      \": XGBRegressor(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample),\n",
    "        \"3. Scikit-Learn \": GradientBoostingRegressor(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample,\n",
    "                    random_state=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_reg_perf(models, X_train, y_train, X_test, y_test)\n",
    "plot_perf(perf_df, \"reg\", \"mercedes\", ylim=(0.0, 0.57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prune_ratio(models, \"reg\", \"mercedes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_lr(models, \"reg\", \"mercedes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Task - Communities Crimes\n",
    "\n",
    "Dataset from UCI: [https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/uci-communities/communities.txt\", header=None, na_values=\"?\")\n",
    "col_names = data.columns\n",
    "col_names_x = col_names[:-1]\n",
    "col_name_y = col_names[-1]\n",
    "X = pp.simple_pp(data[col_names_x]).values\n",
    "y = data[col_name_y].values\n",
    "print(\"- Avg(y): {}, Std(y): {}\".format(np.mean(y), np.std(y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size = test_size,\n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"0. PaloBoost    \": PaloBoost(distribution=\"gaussian\",\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "        \"1. SGTB-Bonsai\": GBM(distribution=\"gaussian\",\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "        \"2. XGBoost      \": XGBRegressor(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_reg_perf(models, X_train, y_train, X_test, y_test)\n",
    "plot_perf(perf_df, \"reg\", \"communities\", ylim=(0.0, 0.68))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prune_ratio(models, \"reg\", \"communities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_lr(models, \"reg\", \"communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task - Amazon Access Challenge\n",
    "\n",
    "We use the dataset from [one of the Kaggle competitions - Amazon.com Employee Access Challenge](https://www.kaggle.com/c/amazon-employee-access-challenge). We minimally preprocess the data - for more information about how we preprocess the data, please read `preprocess.py` in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/kaggle-amazon/train.csv\")\n",
    "col_names_x = list(set(data.columns)-set([\"ACTION\"]))\n",
    "for cname in col_names_x:\n",
    "    data[cname] = data[cname].astype(\"object\")\n",
    "X = pp.simple_pp(data[col_names_x]).values\n",
    "y = data[\"ACTION\"].values\n",
    "print(\"- Avg(y): {}\".format(np.mean(y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size = test_size,\n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"0. PaloBoost    \": PaloBoost(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"1. SGTB-Bonsai\": GBM(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"2. XGBoost      \": XGBClassifier(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample),\n",
    "         \"3. Scikit-Learn \": GradientBoostingClassifier(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample,\n",
    "                    random_state=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_cls_perf(models, X_train, y_train, X_test, y_test)\n",
    "plot_perf(perf_df, \"cls\", \"amazon\", ylim=(0.58, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prune_ratio(models, \"cls\", \"amazon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_lr(models, \"cls\", \"amazon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task - Pulsar Detection\n",
    "\n",
    "Pulsar detection. UCI dataset from [https://archive.ics.uci.edu/ml/datasets/HTRU2](https://archive.ics.uci.edu/ml/datasets/HTRU2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/uci-htru/HTRU_2.csv\", header=None)\n",
    "col_names = data.columns\n",
    "col_names_x = col_names[:-1]\n",
    "col_name_y = col_names[-1]\n",
    "X = pp.simple_pp(data[col_names_x]).values\n",
    "y = data[col_name_y].values\n",
    "print(\"- Avg(y): {}\".format(np.mean(y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size = test_size,\n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"0. PaloBoost    \": PaloBoost(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"1. SGTB-Bonsai\": GBM(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"2. XGBoost      \": XGBClassifier(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample),\n",
    "         \"3. Scikit-Learn\": GradientBoostingClassifier(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample,\n",
    "                    random_state=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_cls_perf(models, X_train, y_train, X_test, y_test)\n",
    "plot_perf(perf_df, \"cls\", \"htru\", ylim=(0.85, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prune_ratio(models, \"cls\", \"htru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_lr(models, \"cls\", \"htru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task - Carvana Don't Get Kicked! Challenge\n",
    "\n",
    "We use the dataset from [one of the Kaggle competitions - Carvana Don't Get Kicked! Challenge](https://www.kaggle.com/c/DontGetKicked). We minimally preprocess the data - for more information about how we preprocess the data, please read `preprocess.py` in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/kaggle-carvana/training.csv\")\n",
    "y = data[\"IsBadBuy\"].values\n",
    "data = data.drop([\"RefId\", \"IsBadBuy\", \"PurchDate\"], axis=1)\n",
    "X = pp.simple_pp(data).values\n",
    "print(\"- Avg(y): {}\".format(np.mean(y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size = test_size,\n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"0. PaloBoost    \": PaloBoost(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"1. SGTB-Bonsai\": GBM(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"2. XGBoost      \": XGBClassifier(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_cls_perf(models, X_train, y_train, X_test, y_test)\n",
    "plot_perf(perf_df, \"cls\", \"carvana\", ylim=(0.65, 0.78))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prune_ratio(models, \"cls\", \"carvana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_lr(models, \"cls\", \"carvana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task - BNP Paribas Challenge\n",
    "\n",
    "We use the dataset from [one of the Kaggle competitions - BNP Paribas Cardif Claims Management](https://www.kaggle.com/c/bnp-paribas-cardif-claims-management). We minimally preprocess the data - for more information about how we preprocess the data, please read `preprocess.py` in the same folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/kaggle-bnp/train.csv\")\n",
    "y = data[\"target\"].values\n",
    "data = data.drop([\"ID\", \"target\"], axis=1)\n",
    "X = pp.simple_pp(data).values\n",
    "print(\"- Avg(y): {}\".format(np.mean(y)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size = test_size,\n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"0. PaloBoost    \": PaloBoost(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"1. SGTB-Bonsai\": GBM(distribution=\"bernoulli\",\n",
    "                        n_estimators=n_estimators, \n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "         \"2. XGBoost      \": XGBClassifier(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_cls_perf(models, X_train, y_train, X_test, y_test)\n",
    "plot_perf(perf_df, \"cls\", \"bnp\", ylim=(0.64, 0.76))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prune_ratio(models, \"cls\", \"bnp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_lr(models, \"cls\", \"bnp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Task - Friedman Simulated Data\n",
    "\n",
    "[http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_friedman1\n",
    "X, y = make_friedman1(n_samples=10000, noise=5, random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                            test_size = test_size,\n",
    "                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"0. PaloBoost    \": PaloBoost(distribution=\"gaussian\",\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "        \"1. SGTB-Bonsai\": GBM(distribution=\"gaussian\",\n",
    "                        n_estimators=n_estimators,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth, \n",
    "                        subsample=subsample),\n",
    "        \"2. XGBoost      \": XGBRegressor(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample),\n",
    "        \"3. Scikit-Learn \": GradientBoostingRegressor(\n",
    "                    n_estimators=n_estimators, \n",
    "                    learning_rate=learning_rate,\n",
    "                    max_depth=max_depth, \n",
    "                    subsample=subsample,\n",
    "                    random_state=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = get_reg_perf(models, X_train, y_train, X_test, y_test)\n",
    "plot_perf(perf_df, \"reg\", \"friedman\", ylim=(0.0,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prune_ratio(models, \"reg\", \"friedman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg_lr(models, \"reg\", \"friedman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = {\"colname\": [\"x{}\".format(i) for i in np.arange(X.shape[1])]}\n",
    "for name, model in models.items():\n",
    "    fi[name] = model.feature_importances_\n",
    "fi = pd.DataFrame(fi)\n",
    "fi = fi.set_index(\"colname\")\n",
    "ax = fi.plot.bar(subplots=True, figsize=(14, 4), layout=(1,4), legend=False)[0][0]\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"figures/{}_{}_ss{}_lr{}_fi.pdf\".format(\"reg\", \"friedman\", subsample, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_staged = defaultdict(list)\n",
    "for stage, fi in enumerate(models[\"0. PaloBoost    \"].get_staged_feature_importances()):\n",
    "    fi_staged[\"stage\"].append(stage)\n",
    "    for j, fi_j in enumerate(fi):\n",
    "        fi_staged[\"x{}\".format(j)].append(fi_j)\n",
    "fi_staged = pd.DataFrame(fi_staged)\n",
    "fi_staged = fi_staged.set_index(\"stage\")\n",
    "ax = fi_staged.plot()\n",
    "ax.set(xlabel=\"Prediction stage\", ylabel=\"Feature importance\")\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"figures/{}_{}_ss{}_lr{}_fi_staged0.pdf\".format(\"reg\", \"friedman\", subsample, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_staged = defaultdict(list)\n",
    "for stage, fi in enumerate(models[\"1. SGTB-Bonsai\"].get_staged_feature_importances()):\n",
    "    fi_staged[\"stage\"].append(stage)\n",
    "    for j, fi_j in enumerate(fi):\n",
    "        fi_staged[\"x{}\".format(j)].append(fi_j)\n",
    "fi_staged = pd.DataFrame(fi_staged)\n",
    "fi_staged = fi_staged.set_index(\"stage\")\n",
    "ax = fi_staged.plot()\n",
    "ax.set(xlabel=\"Prediction stage\", ylabel=\"Feature importance\")\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(\"figures/{}_{}_ss{}_lr{}_fi_staged1.pdf\".format(\"reg\", \"friedman\", subsample, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speed benchmark\n",
    "if False: \n",
    "    n_list = [1000, 5000, 10000, 50000, 100000, 500000, 1000000]\n",
    "    speed_df = defaultdict(list)\n",
    "    for n_sample in n_list:\n",
    "        X, y = make_friedman1(n_samples=n_sample, noise=5, random_state=0)\n",
    "        speed_df[\"n_sample\"].append(n_sample)\n",
    "        for name, model in models.items():\n",
    "            start_time = time.time()\n",
    "            model.fit(X, y)\n",
    "            fit_time = time.time() - start_time\n",
    "            speed_df[name].append(fit_time)\n",
    "            print(name, n_sample, fit_time)\n",
    "    speed_df = pd.DataFrame(speed_df)\n",
    "    speed_df = speed_df.set_index(\"n_sample\")\n",
    "    ax = speed_df.plot(style=[\".-\",\".--\",\".:\",\".-.\"], logx=True, logy=True, figsize=(6, 4))\n",
    "    ax.set(xlabel=\"Number of Samples\", ylabel=\"Training Time (in Seconds)\")\n",
    "    fig = ax.get_figure()\n",
    "    fig.savefig(\"figures/{}_{}_ss{}_lr{}_speed.pdf\".format(\"reg\", \"friedman\", subsample, learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
